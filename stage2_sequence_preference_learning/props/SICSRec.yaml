hidden_act: 'gelu'
loss_type: 'CE'
plm_suffix: pth
img_suffix: pth

text_size: 768
image_size: 1000
plm_text_size: 768
plm_image_size: 1000

seed: 2024

n_layers: 2
n_heads: 2

initializer_range: 0.02
layer_norm_eps: 1e-12

hidden_size: 256
inner_size: 256

hidden_dropout_prob: 0.5
attn_dropout_prob: 0.5
item_dropout: 0.5

learning_rate: 0.001
train_batch_size: 1024
temperature: 0.05
lora_rank: 4
stage: stage1
lamba: 0.01
alpha: 0.2
gpu_id: 1

